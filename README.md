# Attendance ETL Pipeline using PySpark & PostgreSQL

## ğŸ“Œ Project Overview
This project implements an end-to-end ETL pipeline for processing attendance data
using PySpark and PostgreSQL. The pipeline reads structured CSV data, applies
transformations, and loads the results into staging and data warehouse tables.

The solution simulates a real-world analytics workflow commonly used in ERP /
data engineering systems.

---

## ğŸ› ï¸ Technology Stack
- PySpark
- PostgreSQL
- JDBC Connectivity
- Python
- Hadoop (local setup)

---

## ğŸ“‚ Project Structure

attendance-project/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ attendance.csv
â”‚
â”œâ”€â”€ drivers/
â”‚   â””â”€â”€ postgresql-42.7.8.jar
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ load_stage_data.py
â”‚   â””â”€â”€ etl_stage_to_dw.py
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

---

## âš™ï¸ ETL Workflow

### 1ï¸âƒ£ Stage Load (`load_stage_data.py`)
This script performs the initial ingestion of attendance records.

### Key Operations:
âœ” Configure Hadoop & Spark environment  
âœ” Define structured schema for CSV  
âœ” Load attendance dataset into Spark DataFrame  
âœ” Apply datatype conversions  
âœ” Load data into PostgreSQL staging table  

### Output:
Database: `stage_db`  
Table: `attendance_project`

---

### 2ï¸âƒ£ Stage â†’ Data Warehouse (`etl_stage_to_dw.py`)
This script performs analytical transformations and warehouse loading.

### Key Operations:
âœ” Read attendance dataset  
âœ” Apply datatype conversions  
âœ” Load Stage Table (`attendance_stage`)  
âœ” Apply business transformation rules  
âœ” Load Data Warehouse Table (`attendance_dw`)

### Example Transformation:
Temperature alerts are derived using rule-based logic:

- If `Temperature_Status = High` â†’ `Temp_Alert = YES`
- Otherwise â†’ `Temp_Alert = NO`

---

## ğŸ—‚ï¸ Dataset Description
The attendance dataset includes:

- Student identity attributes
- Attendance metadata
- Check-in timestamps
- Temperature measurements
- Verification details
- Correction / source tracking fields

---

##  How to Run

### Step 1 â€” Start PostgreSQL
Ensure PostgreSQL is running and database is created

### Step 2 â€” Verify JDBC Driver
Place PostgreSQL JDBC driver inside

### Step 3 â€” Run Stage Load

### Step 4 â€” Run ETL Pipeline



##  Learning Outcomes
This project demonstrates practical concepts in:

âœ” Data Engineering  
âœ” ETL Pipeline Design  
âœ” Schema Definition  
âœ” Spark Transformations  
âœ” JDBC Integration  
âœ” Staging vs Data Warehouse Layers  

---

##  Future Enhancements
- Incremental data loads
- Data validation checks
- Fact & Dimension modelling
- Performance optimization
- BI dashboard integration (Power BI)

---

# Shalini Janarthanan  
B.Tech â€“ Artificial Intelligence & Data Science

